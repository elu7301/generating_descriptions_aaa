{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2480b83fe4954a0fb69daeb0e4efc5c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59dee7ef3d154ff79c828dc74d600cac","IPY_MODEL_c69228df930f4540b6570ff843dbd8fa","IPY_MODEL_74fe5aecb72a45ab860b89fa93be942c"],"layout":"IPY_MODEL_c172c06d5b074ee6beac0e64d39fdb2b"}},"59dee7ef3d154ff79c828dc74d600cac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea2ecb433de143c19be6ff1fa902660c","placeholder":"​","style":"IPY_MODEL_9d671165a3234e0186fc5d8b4d3233d4","value":"Loading checkpoint shards:  50%"}},"c69228df930f4540b6570ff843dbd8fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7767ce61be9c42cab9d7cc779e2a0516","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_143cf519eacc4893a6407a9a3a94fc19","value":2}},"74fe5aecb72a45ab860b89fa93be942c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_597e701568714fdf9d0a783c1bc789ed","placeholder":"​","style":"IPY_MODEL_b354ea3020384d3baaeac006f69c9f28","value":" 2/4 [01:07&lt;01:07, 33.85s/it]"}},"c172c06d5b074ee6beac0e64d39fdb2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea2ecb433de143c19be6ff1fa902660c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d671165a3234e0186fc5d8b4d3233d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7767ce61be9c42cab9d7cc779e2a0516":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143cf519eacc4893a6407a9a3a94fc19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"597e701568714fdf9d0a783c1bc789ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b354ea3020384d3baaeac006f69c9f28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8593898,"sourceType":"datasetVersion","datasetId":5140935},{"sourceId":45318,"sourceType":"modelInstanceVersion","modelInstanceId":37999}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ( VisionEncoderDecoderModel, \n                          ViTFeatureExtractor, \n                          BitsAndBytesConfig,\n                          Seq2SeqTrainer,\n                          Seq2SeqTrainingArguments\n                         )\n\nfrom peft import LoraConfig, get_peft_model\nimport torch\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import functional as F\nfrom PIL import Image\nimport requests\nimport pandas as pd\nimport json\nimport os\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:30:53.443803Z","iopub.execute_input":"2024-06-09T12:30:53.444236Z","iopub.status.idle":"2024-06-09T12:31:15.679791Z","shell.execute_reply.started":"2024-06-09T12:30:53.444203Z","shell.execute_reply":"2024-06-09T12:31:15.678934Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-06-09 12:31:02.101260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-09 12:31:02.101373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-09 12:31:02.248907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"vit = VisionEncoderDecoderModel.from_pretrained(\"tuman/vit-rugpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"tuman/vit-rugpt2-image-captioning\")\n\ntokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"tuman/vit-rugpt2-image-captioning\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:31:15.680898Z","iopub.execute_input":"2024-06-09T12:31:15.681505Z","iopub.status.idle":"2024-06-09T12:32:07.891998Z","shell.execute_reply.started":"2024-06-09T12:31:15.681477Z","shell.execute_reply":"2024-06-09T12:32:07.891170Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9e2f3347f5440f8731eddb394d2a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e727af4e184149b6cccf6f0491db7c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51af21b5a5cf47c9a9c8f36d8e385eb9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/780 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecc2d87c51b4ff7a3edb2abaa11605d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e821b75d1514f26a06ab7b89e7d5cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70796b3a47a431bbf10fbdd00ba4194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/217 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e700ee45e94962b66235b9f855ac17"}},"metadata":{}}]},{"cell_type":"code","source":"vit.decoder.config.pad_token_id = 0\nvit.decoder.config.bos_token_id = 1\nvit.decoder.config.eos_token_id = 2","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:32:07.893184Z","iopub.execute_input":"2024-06-09T12:32:07.893479Z","iopub.status.idle":"2024-06-09T12:32:07.898185Z","shell.execute_reply.started":"2024-06-09T12:32:07.893454Z","shell.execute_reply":"2024-06-09T12:32:07.897269Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/avito-desc/aaa_data/items.csv')\nimage_dir = '/kaggle/input/avito-desc/aaa_data/images'","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:32:07.899309Z","iopub.execute_input":"2024-06-09T12:32:07.899597Z","iopub.status.idle":"2024-06-09T12:32:08.993472Z","shell.execute_reply.started":"2024-06-09T12:32:07.899574Z","shell.execute_reply":"2024-06-09T12:32:08.992573Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')  \n    regex = r'[^a-zA-Zа-яА-Я0-9.,:;!?()\\\"\\'\\[\\]{}\\- ]'\n    return re.sub(regex, '', text)\n\ndf['description'] = df['description'].apply(clean_text)\n\n\ndef parse_attrs(attrs):\n    try:\n        return json.loads(attrs.replace(\"'\", '\"'))\n    except json.JSONDecodeError as e:\n        return {}\n    \ndef get_image_paths(image_ids):\n    image_ids_list = image_ids.split(', ')\n    return [os.path.join(image_dir, f\"{img_id}.jpg\") for img_id in image_ids_list][0]\n\ndef preprocess_data(row):\n    title = 'Заголовок: ' + row['title'] + ','\n    attrs = \", \".join([f\"{k}: {v}\" for k, v in row['parsed_attrs'].items()])\n    return f\"{title} {attrs}, Описание: \"\n\ndf.drop(columns=['itemid', 'clothes_type'], inplace=True)\ndf['parsed_attrs'] = df['attrs'].apply(parse_attrs)\n\nattrs_df = pd.json_normalize(df['parsed_attrs'])\ndf = pd.concat([df, attrs_df], axis=1)\ndf['Цена'].fillna(df['Цена'].mean(), inplace=True)\ndf.fillna('', inplace=True)\ndf['image_paths'] = df['images'].apply(get_image_paths)\ndf['input_text'] = df.apply(preprocess_data, axis=1)\ndf = df[['input_text', 'description', 'image_paths']]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-09T12:36:10.532359Z","iopub.execute_input":"2024-06-09T12:36:10.533094Z","iopub.status.idle":"2024-06-09T12:36:13.088161Z","shell.execute_reply.started":"2024-06-09T12:36:10.533061Z","shell.execute_reply":"2024-06-09T12:36:13.087110Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3094951034.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['Цена'].fillna(df['Цена'].mean(), inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"# df['combined_text'] = df['input_text'] + ' ' + df['description']\n# df['tokenized_text'] = df['combined_text'].apply(tokenizer_gpt2)\n# df['tokenized_text'] = df['tokenized_text'].apply(lambda x: x.input_ids)\n# df['token_length'] = df['tokenized_text'].apply(len)\n# max_length = df['token_length'].max()\n# print(\"\\nДлина самого длинного по токенам текста:\", max_length)\n\n# df['tokenized_text'] = df['input_text'].apply(tokenizer_gpt2)\n# df['tokenized_text'] = df['tokenized_text'].apply(lambda x: x.input_ids)\n# df['token_length'] = df['tokenized_text'].apply(len)\n# max_length = df['token_length'].max()\n# print(\"\\nДлина самого длинного по токенам инпута:\", max_length)\n\n# df['tokenized_text'] = df['description'].apply(tokenizer_gpt2)\n# df['tokenized_text'] = df['tokenized_text'].apply(lambda x: x.input_ids)\n# df['token_length'] = df['tokenized_text'].apply(len)\n# max_length = df['token_length'].max()\n# print(\"\\nДлина самого длинного по токенам описания:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:44:25.345807Z","iopub.execute_input":"2024-06-08T15:44:25.346481Z","iopub.status.idle":"2024-06-08T15:44:25.352090Z","shell.execute_reply.started":"2024-06-08T15:44:25.346434Z","shell.execute_reply":"2024-06-08T15:44:25.351053Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df['concat'] = df['input_text'] + ' ' + df['description']","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:36:16.089176Z","iopub.execute_input":"2024-06-09T12:36:16.090007Z","iopub.status.idle":"2024-06-09T12:36:16.129505Z","shell.execute_reply.started":"2024-06-09T12:36:16.089949Z","shell.execute_reply":"2024-06-09T12:36:16.128266Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df = df.head(10000)\n\ntrain_df, eval_df = train_test_split(df, test_size=0.05, random_state=42)\n\ntrain_dataset = Dataset.from_pandas(train_df)\neval_dataset = Dataset.from_pandas(eval_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:36:16.228697Z","iopub.execute_input":"2024-06-09T12:36:16.229682Z","iopub.status.idle":"2024-06-09T12:36:16.244665Z","shell.execute_reply.started":"2024-06-09T12:36:16.229642Z","shell.execute_reply":"2024-06-09T12:36:16.242450Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    concat = examples['concat']\n    img = examples['image_paths']\n    img = [Image.open(x).convert(\"RGB\") for x in img]\n    img =  feature_extractor(img, return_tensors=\"pt\")\n    \n    concat = [text + tokenizer_gpt2.eos_token for text in concat]\n    concat = tokenizer_gpt2(concat, padding='max_length', truncation=True, max_length=350, return_tensors=\"pt\")\n    \n    return {\n        \"labels\": concat[\"input_ids\"],\n        \"pixel_values\": img[\"pixel_values\"],\n    }\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"description\", 'image_paths', 'concat'])\neval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"description\", 'image_paths', 'concat'])\nprint(train_dataset.column_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:36:17.628861Z","iopub.execute_input":"2024-06-09T12:36:17.629737Z","iopub.status.idle":"2024-06-09T12:40:02.717137Z","shell.execute_reply.started":"2024-06-09T12:36:17.629708Z","shell.execute_reply":"2024-06-09T12:40:02.716204Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde2935ab4cb4a288090ab395104379f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97912a0d763c413d9efc046e760b404d"}},"metadata":{}},{"name":"stdout","text":"['__index_level_0__', 'labels', 'pixel_values']\n","output_type":"stream"}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules='all-linear',\n    lora_dropout=0.1,\n)\n\nmodel = get_peft_model(vit, lora_config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    output_dir=\"./results\",\n    logging_dir=\"./logs\",\n    logging_steps=10000000,\n    save_steps=10000000, \n    save_total_limit=2,\n    fp16=True,\n    save_strategy=\"no\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:41:55.794524Z","iopub.execute_input":"2024-06-09T12:41:55.795427Z","iopub.status.idle":"2024-06-09T15:26:03.483862Z","shell.execute_reply.started":"2024-06-09T12:41:55.795391Z","shell.execute_reply":"2024-06-09T15:26:03.482832Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1110: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240609_124203-sxevieol</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/vanya71161rainbow/huggingface/runs/sxevieol' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/vanya71161rainbow/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/vanya71161rainbow/huggingface' target=\"_blank\">https://wandb.ai/vanya71161rainbow/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/vanya71161rainbow/huggingface/runs/sxevieol' target=\"_blank\">https://wandb.ai/vanya71161rainbow/huggingface/runs/sxevieol</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4750' max='4750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4750/4750 2:43:36, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4750, training_loss=0.7748991570723684, metrics={'train_runtime': 9844.5068, 'train_samples_per_second': 0.965, 'train_steps_per_second': 0.483, 'total_flos': 8.604109354917888e+18, 'train_loss': 0.7748991570723684, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"promt = train_df.iloc[7].input_text\nimg = Image.open(train_df.iloc[7].image_paths).convert('RGB')\n\npix = feature_extractor(img, return_tensors=\"pt\")\ntext = tokenizer_gpt2(promt, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:53:51.742774Z","iopub.execute_input":"2024-06-09T15:53:51.743751Z","iopub.status.idle":"2024-06-09T15:53:51.767800Z","shell.execute_reply.started":"2024-06-09T15:53:51.743715Z","shell.execute_reply":"2024-06-09T15:53:51.766833Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"promt","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:53:51.932706Z","iopub.execute_input":"2024-06-09T15:53:51.933018Z","iopub.status.idle":"2024-06-09T15:53:51.939459Z","shell.execute_reply.started":"2024-06-09T15:53:51.932993Z","shell.execute_reply":"2024-06-09T15:53:51.938439Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'Заголовок: Вечернее платье размер 48, Вид одежды: Женская одежда, Вид объявления: Продаю своё, Место сделки: экспериментальный жилой комплекс Мещерское Озеро, Состояние: Хорошее, Цвет: Золотой, Бренд одежды: Другой, Размер. Женская одежда: 48 (L), Предмет одежды: Платья, Описание: '"},"metadata":{}}]},{"cell_type":"code","source":"train_df.iloc[7].description","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:53:52.807025Z","iopub.execute_input":"2024-06-09T15:53:52.807861Z","iopub.status.idle":"2024-06-09T15:53:52.815418Z","shell.execute_reply.started":"2024-06-09T15:53:52.807827Z","shell.execute_reply":"2024-06-09T15:53:52.814401Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'Платье вечернее, ткань похожа на трикотажный велюр или бархат, состав ткани 55 полиэстер, 25 хлопок, 20 спандекс, бу состояние хорошее, размер 48 Длина по спине 98 см ОГ - 99 см ОБ 103 см Высылаю'"},"metadata":{}}]},{"cell_type":"code","source":"res = model.generate(\n    pixel_values=pix.pixel_values.cuda(), \n    decoder_input_ids=text.input_ids.cuda(),\n    max_length=400,\n    eos_token_id=tokenizer_gpt2.eos_token_id,\n#     do_sample=True,\n#     temperature=0.7\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-09T16:08:58.601558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_gpt2.decode(res[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vit.state_dict(), './state_dict')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(r'./state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:34:59.540488Z","iopub.execute_input":"2024-06-09T15:34:59.540846Z","iopub.status.idle":"2024-06-09T15:34:59.549526Z","shell.execute_reply.started":"2024-06-09T15:34:59.540819Z","shell.execute_reply":"2024-06-09T15:34:59.548304Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/state_dict","text/html":"<a href='./state_dict' target='_blank'>./state_dict</a><br>"},"metadata":{}}]}]}